{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq7EJNqpHWTahgfdmHfJcu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VKF14/Lung-Cancer-Detection/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQz7uKQnV2cQ"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-addons==0.16.1\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import random\n",
        "import os\n",
        "import imageio\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.subplots import make_subplots\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, RepeatedStratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
        "from keras.applications import resnet\n",
        "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "McWs_H6TWEZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = r'/content/drive/MyDrive/Project'\n",
        "\n",
        "categories = ['Bengin cases', 'Malignant cases', 'Normal cases']"
      ],
      "metadata": {
        "id": "TDK0zOLzWHxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_data = {}\n",
        "for i in categories:\n",
        "    path = os.path.join(directory, i)\n",
        "    class_num = categories.index(i)\n",
        "    temp_dict = {}\n",
        "    for file in os.listdir(path):\n",
        "        filepath = os.path.join(path, file)\n",
        "        height, width, channels = imageio.imread(filepath).shape\n",
        "        if str(height) + ' x ' + str(width) in temp_dict:\n",
        "            temp_dict[str(height) + ' x ' + str(width)] += 1 \n",
        "        else:\n",
        "            temp_dict[str(height) + ' x ' + str(width)] = 1\n",
        "    \n",
        "    size_data[i] = temp_dict\n",
        "        \n",
        "size_data"
      ],
      "metadata": {
        "id": "scXua7k9WLlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in categories:\n",
        "    path = os.path.join(directory, i)\n",
        "    class_num = categories.index(i)\n",
        "    for file in os.listdir(path):\n",
        "        filepath = os.path.join(path, file)\n",
        "        print(i)\n",
        "        img = cv2.imread(filepath, 0)\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "        break"
      ],
      "metadata": {
        "id": "EsK01zZCWPj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 256\n",
        "for i in categories:\n",
        "    cnt, samples = 0, 3\n",
        "    fig, ax = plt.subplots(samples, 3, figsize=(15, 15))\n",
        "    fig.suptitle(i)\n",
        "    \n",
        "    path = os.path.join(directory, i)\n",
        "    class_num = categories.index(i)\n",
        "    for curr_cnt, file in enumerate(os.listdir(path)):\n",
        "        filepath = os.path.join(path, file)\n",
        "        img = cv2.imread(filepath, 0)\n",
        "        \n",
        "        img0 = cv2.resize(img, (img_size, img_size))\n",
        "        img1 = cv2.GaussianBlur(img0, (5, 5), 0)\n",
        "        \n",
        "        ax[cnt, 0].imshow(img)\n",
        "        ax[cnt, 1].imshow(img0)\n",
        "        ax[cnt, 2].imshow(img1)\n",
        "        cnt += 1\n",
        "        if cnt == samples:\n",
        "            break\n",
        "        \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iZgl69CEWWaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "img_size = 256\n",
        "\n",
        "for i in categories:\n",
        "    path = os.path.join(directory, i)\n",
        "    class_num = categories.index(i)\n",
        "    for file in os.listdir(path):\n",
        "        filepath = os.path.join(path, file)\n",
        "        img = cv2.imread(filepath, 0)\n",
        "        # preprocess here\n",
        "        img = cv2.resize(img, (img_size, img_size))\n",
        "        data.append([img, class_num])\n",
        "        random.shuffle(data)\n",
        "\n",
        "X, y = [], []\n",
        "for feature, label in data:\n",
        "    X.append(feature)\n",
        "    y.append(label)\n",
        "    \n",
        "print('X length:', len(X))\n",
        "print('y counts:', Counter(y))\n",
        "\n",
        "# normalize\n",
        "X = np.array(X).reshape(-1, img_size, img_size, 1)\n",
        "X = X / 255.0\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "YXHZylAiWXHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=10, stratify=y)\n",
        "\n",
        "print(len(X_train), X_train.shape)\n",
        "print(len(X_valid), X_valid.shape)"
      ],
      "metadata": {
        "id": "Fz2XtAtZWcXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(y_train), Counter(y_valid))"
      ],
      "metadata": {
        "id": "tEr8TxwaWj62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train), X_train.shape)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_size*img_size*1)\n",
        "\n",
        "print(len(X_train), X_train.shape)"
      ],
      "metadata": {
        "id": "ayhoZo-RWmpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Before SMOTE:', Counter(y_train))\n",
        "smote = SMOTE()\n",
        "X_train_sampled, y_train_sampled = smote.fit_resample(X_train, y_train)\n",
        "print('After SMOTE:', Counter(y_train_sampled))"
      ],
      "metadata": {
        "id": "NKyEBWlfWplq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], img_size, img_size, 1)\n",
        "X_train_sampled = X_train_sampled.reshape(X_train_sampled.shape[0], img_size, img_size, 1)\n",
        "\n",
        "print(len(X_train), X_train.shape)\n",
        "print(len(X_train_sampled), X_train_sampled.shape)"
      ],
      "metadata": {
        "id": "mJgSfWUvWuj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Conv2D(64, (3, 3), input_shape=X_train.shape[1:]))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(16))\n",
        "model1.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model1.summary()"
      ],
      "metadata": {
        "id": "tNl2lNcjWyqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "TIad0iZsW2Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model1.fit(X_train_sampled, y_train_sampled, batch_size=8, epochs=50, validation_data=(X_valid, y_valid))"
      ],
      "metadata": {
        "id": "rHwefEJTW6om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model1.predict(X_valid, verbose=1)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(classification_report(y_valid, y_pred_bool))\n",
        "\n",
        "print(confusion_matrix(y_true=y_valid, y_pred=y_pred_bool))"
      ],
      "metadata": {
        "id": "KVzKCAHiW9nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='Train')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EhUe6soBXBVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.save('model.h5')"
      ],
      "metadata": {
        "id": "iZX5pOx7XEJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the Keras model from the .h5 file\n",
        "model = tf.keras.models.load_model('model.h5')\n",
        "\n",
        "# Convert the Keras model to a pickle object\n",
        "model_pickle = pickle.dumps(model)\n",
        "\n",
        "# Save the pickle object to a file\n",
        "with open('my_model.pickle', 'wb') as f:\n",
        "    f.write(model_pickle)"
      ],
      "metadata": {
        "id": "qpUA-la0XJQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "mbad-8gUXMD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the image using OpenCV\n",
        "image_path = input(\"Enter path to image: \")\n",
        "img = cv2.imread(image_path , 0)\n",
        "\n",
        "# Preprocess the image\n",
        "img = cv2.resize(img, (img_size, img_size))\n",
        "img = img / 255.0\n",
        "img = np.array(img).reshape(-1, img_size, img_size, 1)\n",
        "\n",
        "# Obtain the prediction from the model\n",
        "prediction = model1.predict(img)\n",
        "\n",
        "# Interpret the prediction\n",
        "class_label = np.argmax(prediction)\n",
        "print('According to the scan image provided the case is', categories[class_label])\n"
      ],
      "metadata": {
        "id": "DDB6xzbmXPI3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}